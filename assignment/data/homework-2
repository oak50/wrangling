# QUESTION 2

#!/usr/bin/env python3
import pandas as pd
import numpy as np # Import the numpy package into your workspace
import matplotlib.pyplot as plt # Import matplotlib into your workspace

df = pd.read_csv('assignment/data/airbnb_hw.csv',low_memory=False)
print(df.shape, '\n') # List the dimensions of df
print(df.head(n=100))

# 1. clean the price column
price = df['Price'] # creates a data frame with all values of the price variable
print(price.unique()) # prints all of the unique values within the price dataframe

# drop null values
count_before_drop = len(df)
df = df.dropna(subset=['Price'])
count_after_drop = len(df)
print( "count before drop: \n", count_before_drop)
print( "count after drop: \n", count_after_drop)
# no null values

# remove commas from prices
df['new_prices'] = pd.to_numeric(df['Price'], errors='coerce') # Coerce the variable to numeric
# Create a missing dummy:
df['price_nan'] = df['Price'].isnull() # Equals 1 if missing, 0 if non-null
# After coercion:
print('After coercion: \n', df['Price'].describe(),'\n') # Describe the numeric variable
print('Total Missings: \n', sum(df['price_nan']),'\n') # How many missing values are there? Apparently 0
new_prices = df['new_prices'] 
print(new_prices.unique())

# 2. clean the "type" variable in the shark dataset
df2 = pd.read_csv('assignment/data/sharks.csv', low_memory=False)
print(df.shape, '\n') # List the dimensions of df
print(df2.head(n=100))

type = df2['Type']
print(type.unique()) # see the unique values. It looks like some are null, so drop those occurances

# drop null values
typecount_before_drop = len(df2)
df2 = df2.dropna(subset=['Type'])
typecount_after_drop = len(df2)
print( "count before drop: \n", typecount_before_drop) # returned 6462
print( "count after drop: \n", typecount_after_drop) # returned 6457 (meaning that 5 values of type were null, and were thus droppe

# 3. For the pretrial data, clean the WhetherDefendantWasReleasedPretrial variable & replace 
# missing values with np.nan

df3 = pd.read_csv('pretrial_data.csv',low_memory=False)
print(df3.shape, '\n') # List the dimensions of df3
df3 = df.rename(columns = {'WhetherDefendantWasReleasedPretrial':'whether_release'})
df3['whether_release'] = df3['whether_release'].fillna(np.nan) # replaces null values of WhetherDefendantWasReleasedPretrial
whetherrelease = df3['WhetherDefendantWasReleasedPretrial']
print(whetherrelease.unique())


# 4.
df = pd.read_csv('pretrial_data.csv',low_memory=False)
print(df.shape, '\n') # List the dimensions of df
print(df.head(n=100))

# CLEAN
df_new = df.rename(columns = {'ImposedSentenceAllChargeInContactEvent':'Imposed'}) # RENAME THE VARIABLE
print(df_new.head(n=100)) # CHECK TO SEE IF IT WORKED
imposed = df_new['Imposed'] # create dataframe with just 1 var
print(imposed.unique()) # check to see unique values -- > see if anything sticks out
# i notice that at least one value is an empty string, and also some numbers have a decimal that seems too long for our purposes
# drop null values:
count_before_drop = len(df_new)
df_new = df_new.dropna(subset=['Imposed'])
count_after_drop = len(df_new)
imposed = df_new['Imposed']
print( "count before drop: \n", count_before_drop)
print( "count after drop: \n", count_after_drop)
# since this didn't do anything, now drop values that are equal to a single space
count_before_drop = len(df_new) # take inital count of the length of the dataframe
df_new['Imposed'] = df_new['Imposed'].replace(' ', np.nan) # replace values " " with np.nan
df_new = df.dropna() # drop null values

print(df_new.head(n=100)) # print the new dataframe
print( "count before drop: \n", count_before_drop) # print counts and compare 
print( "count after drop: \n", count_after_drop) # looks like a lot of values were dropped. If i did this right, it means that a lot of values were spaces


# QUESTION 3

# 1. How did the most recent US Census gather data on race? 
      # --> The US census gathers racial demographic data based on self-reporting.

# 2. Why do we gather these data? What role do these kinds of data play in politics and society? Why does data quality matter? 
      # --> Census data helps to inform social policy and funding allocation strategies. For instance, if the census identifies that a large
      # number of Americans are youth or elderly, then this might influence funding allocations to childcare or social security. Additionally, 
      # Census data is used to calculate the number of representives needed for local and state governments. Data 
      # quality is important because social groups and agencies need accurate data in order to inform their strategies.

# 3. Please provide a constructive criticism of how the Census was conducted: What was done well? What do you think was missing? 
# How should future large scale surveys be adjusted to best reflect the diversity of the population? Could some of the Census' 
# good practices be adopted more widely to gather richer and more useful data?
      # --> Audits discovered that the Census over- and under-counted multiple state populations populations, as well as counts for certain 
      # racial groups (including hispanics). One measure used by the Census, called the post-enumeration survey, excludes group housing 
      # locations such as dorms and prisons from its sample, which could lead to possible undercounting. However, the Census does 
      # incorporate multiple forms of sampling (including a survey portion and a portion which utilizes birth and death records). This 
      # multi-analysis framework could be applied to other large sample surveys. In order to fully incorporate demographic diversity within
      # sample results, Census data scientists should ensure that all US communities are reached by the census porportionally, through 
      # modes such as mail, phone, and door-to-door sampling. If some demographics are harder to reach through these sampling methods, then
      # the census should alter their methods to ensure that sampling modes are accessible.

# 4. How did the Census gather data on sex and gender? Please provide a similar constructive criticism of their practices.
      # --> The Census prompts survey participants to identify their sex (with options of male and female). This question design excludes
      # individuals who are intersex, trans, and nonbinary. In the future, additional questions could be added which allow participants to
      # identify their gender identity.

#5. When it comes to cleaning data, what concerns do you have about protected characteristics like sex, gender, sexual identity, 
# or race? What challenges can you imagine arising when there are missing values? What good or bad practices might people adopt, and why?
      # --> During the cleaning process, information such as secondary racial identities might be removed from a participant's profile
      # for the sake of categorization. For instance, even if a person has partial Native American ancestory, their racial profile
      # in the census could ommit this information. On an aggregate level, this could result in undercounting of minority groups, which
      # affects the amount of services that groups receive. Furthermore, if characteritics such as sex or gender identity are missing 
      # from samples, data scientists might either ommit these data points or replace them with predictions. For instance, if an 
      # intersex person chose not to list their sex on the census, then their response would be listed as null, which is nondescriptive 
      # of their actual sex.

#6. Suppose someone invented an algorithm to impute values for protected characteristics like race, gender, sex, or sexuality. 
# What kinds of concerns would you have?
      # This algorithm might inaccurately apply other data listed about the subject (such as their income or residence) to make 
      # a judgement on their expected race or gender. This could lead to misrepresentation of overall demographics.
